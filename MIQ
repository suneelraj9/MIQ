VXVM
******
>> Veritas Naming Scheme
  1. Operating system-based naming scheme
  2. Enclosure based naming scheme.
  
  bash-3.00# vxddladm get namingscheme
  NAMING_SCHEME   PERSISTENCE   LOWERCASE       USE_AVID
  ======================================================
  Enclosure Based      Yes        Yes            Yes
  bash-3.00#
  
>> To change the Operating system based naming scheme
   bash-3.00# vxddladm set namingscheme=osn
   bash-3.00# vxdisk list
   DEVICE    TYPE     DISK    GROUP      STATUS
   c1t0d0s2 auto:ZFS   -       -       ZFS
   c1t3d0   auto:none  -       -       online invalid
   c1t4d0   auto:ZFS   -       -       ZFS
   c1t5d0   auto:none  -       -       online invalid
   
>> You can match the OS native names(i.e format) with VXVM enclose based names using the below command
   bash-3.00# vxdisk -e list
   DEVICE  TYPE   DISK   GROUP STATUS  OS_NATIVE_NAME ATTR
   disk_0  auto:none    -   -  online invalid  c1t3d0     -
   disk_1  auto:none    -   -  online invalid  c1t5d0     -
   disk_2  auto:ZFS     -   -  ZFS     c1t4d0     -
   disk_3  auto:ZFS     -   -  ZFS     c1t0d0s2   -

 >> Creating the new disk group
    #vxdg init UXDG UXDISK1=disk_0 UXDISK2=disk_1
    #vxdisk list
    DEVICE     TYPE             DISK     GROUP   STATUS
    disk_0     auto:cdsdisk    UXDISK1   UXDG    online
    disk_1     auto:cdsdisk    UXDISK2   UXDG    online
    
 >> After scanning the disk in OS,Scan the disks in veritas level by using the below command.
    bash-3.00# vxdisk scandisks
    
 >> Bringing the disk in  to veritas control
    bash-3.00# /etc/vx/bin/vxdisksetup -i disk_0
    
 >> If you want to setup the disk in specific format,you can use the below syntax.
    #vxdisksetup -i disk_XX format=cds or simple or sliced
    
 >> Addition of disk
    #vxdg -g UXDG adddisk UXDISK3=disk_4
 
 >> Removing the disk
    #vxdg -g UXDG rmdisk UXDISK3
 
 >> Maximum size of volume
    #vxassist -g UXDG maxsize
    Maximum volume size: 284672 (139Mb)
    
    #vxassist -g UXDG maxsize layout=mirror
    Maximum volume size: 10240 (60Mb)
    
    #vxassist -g UXDG maxsize layout=stripe
    Maximum volume size: 284672 (139Mb)
    
 >> Deporting diskgroup
    #vxdg list
    NAME      STATE        ID
    UXDG      enabled,cds       1364022395.37.sfos
    
    #vxdg deport UXDG
    By default vxdisk output will not show the deported diskgroup tag
    
    #vxdisk list
    DEVICE    TYPE           DISK  GROUP  STATUS
    disk_0    auto:cdsdisk    -     -     online
    disk_1    auto:cdsdisk    -     -     online
    disk_2    auto:ZFS        -     -     ZFS
    disk_3    auto:ZFS        -     -     ZFS
    disk_4    auto:cdsdisk    -     -     online
    disk_5    auto:cdsdisk    -     -     online
    disk_6    auto:cdsdisk    -     -     online
    
  >> To find the deported diskgroup disks
    #vxdisk -o alldgs list
    DEVICE    TYPE           DISK   GROUP  STATUS
    disk_0    auto:cdsdisk    -     (UXDG) online
    disk_1    auto:cdsdisk    -     (UXDG) online
    
  >> Importing the deported diskgroup
    #vxdg import UXDG
    #vxdg list
    NAME     STATE   ID
    UXDG     enabled,cds  1364022395.37.sfos
   
   Sometimes you may need to use -C flag to import the cluster diskgroup to clear the VCS lock.

   # vxdg -C import DG_NAME
   
 >> Re-Naming the Diskgroup
    #vxdg deport UXDG
    #vxdg -n NEWDG import UXDG
    
 >> Diskgroup configuration backup/restore:
    # ls -lrt /etc/vx/cbr/bk
    total 6
    drwxr-xr-x  14 root  root 18 Mar 27 19:18 UXDG.1364022395.37.sfos
    drwxr-xr-x   2 root  root  6 Mar 27 21:21 UXDG.1364398489.45.sfos
    
 >> To take current configuration backup
    # /etc/vx/bin/vxconfigbackup
    
 >> To take the current configuration to specific location
     # /etc/vx/bin/vxconfigbackup -l /var/tmp
    Start backing up diskgroup UXDG to /var/tmp/UXDG.1364398489.45.sfos.......
    VxVM  NOTICE V-5-2-3100 Backup complete for diskgroup: UXDG
    # ls -lrt /var/tmp/UXDG.1364398489.45.sfos
    total 48473 
    -rw-r--r-- 1 root root Mar28 17:38 1364398489.45.sfos.diskinfo
    -rw-r--r-- 1 root root Mar28 17:38 1364398489.45.sfos.cfgrec
    -rw-r--r-- 1 root root Mar28 17:39 1364398489.45.sfos.binconfig
    -rw-r--r-- 1 root root Mar28 17:39 1364398489.45.sfos.dginfo
    
 >> Display the configuration file in readable format
    #cd /etc/vx/cbr/bk/UXDG.1364474089.57.sfos
    # cat 1364474089.57.sfos.cfgrec |vxprint -D - -htr
    Disk group: UXDG
    dg UXDG  default      default  25000    1364474089.57.sfos
    dm UXDISK1      disk_0       auto     65536    143056   -
    dm UXDISK2      disk_1       auto     65536    143056   -
    
 >> To restore the diskgroup configuration
    To pre-commit the changes run the following  
    #/etc/vx/bin/vxconfigrestore -p UXDG

    To commit the changes after the precommit 
    #/etc/vx/bin/vxconfigrestore -c UXDG

    To abort the pre-commit: 
    #/etc/vx/bin/vxconfigrestore -d UXDG
    
 >> To confirm that disk group is ok:   
    #vxprint -g UXDG
    
 >> Creating new volume
    Syntax:
    #vxassist -g diskgroup make vol_name size layout=format disk-name
    
    > Concatenation volume
      # vxassist -g UXDG make uxoravol1 50M
    > If you want to make the volume using the specific disk
      bash-3.00# vxassist -g UXDG make uxoravol1 50M UXDISK1
    > use mkfs to create vxfs filesystem
      # mkfs -F vxfs /dev/vx/rdsk/UXDG/uxoravol1
    > create a new mountpoint and mount
      mount -F vxfs /dev/vx/dsk/UXDG/uxoravol1 /uxoravol1
      
  >> Striped volume
     #vxassist -g UXDG make urstripe 100M UXDISK1 UXDISK2 UXDISK3 layout=stripe  stripeunit=32k
     
  >> Mirror volume
     # vxassist -g UXDG make uxvol2 50M layout=mirror
     
     volume with two plex – one way mirror
     volume with three plex – two way mirror.
     
  >> Mirrored-stripe or RAID-0+1(stripping + mirroring)
     # vxassist -g UXDG make msvol 50M layout=mirror-stripe
     
  >> Striped-mirror or RAID- 1+0 (mirroring +stripping)- Layered volume
     # vxassist -g UXDG make smvol 50M layout=stripe-mirror
     
 *************************************************************
                  VCS-Veritas Cluster
 *************************************************************
 
 VCS
 ****
   > Failover – Service group runs on one system at a time.
   > Parallel – Service group runs on multiple systems simultaneously
   > Hybrid – SG behaves as Failover within the local cluster and Parallel for the remote cluster.
   
   > VCS main configuration file located -> /etc/VRTSvcs/conf/config/main.cf
   
   > set VCS configuration file (main.cf) ro/rw
     # haconf -dump -makero 
     # haconf -makerw   
     
   > VCS engine log file located -> /var/VRTSvcs/log/engine_A.log.
     #hamsg engine_A
     
   > check the status of the entire cluster
    #hastatus -sum
    
   > verify the syntax of the main.cf 
     # hacf -verify /etc/VRTSvcs/conf/config
     
 Different resource types
 *************************
     1. Persistent : VCS can only monitor these resources but can not offline or online them.
     2. On-Off : VCS can start and stop On-Off resource type. Most resources fall in this category.
     3. On-Only : VCS starts On-Only resources but does not stop them.Example:NFS
     
 steps involved in Offline VCS configuration
 ********************************************
    # haconf -dump -makero
    # hastop -all
    # cp -p /etc/VRTSvcs/conf/config/main.cf /etc/VRTSvcs/conf/config/main.cf_17march
    # vi /etc/VRTSvcs/conf/config/main.cf
    # hacf -verify /etc/VRTSvcs/conf/config/
    # hastart
    
 GAB, LLT and HAD 
 *****************
   Basic building blocks of vcs functionality.
   LLT (low latency transport protocol) – LLT transmits the heartbeats over the interconnects.
   GAB (Group membership services and atomic broadcast) - overall cluster membership information by tracking the heartbeats sent over LLT interconnects
   HAD (High Availability daemon) – the main VCS engine which manages the agents and service group. It is in turn monitored by a daemon named hashadow
   
 > Check the status of various GAB ports on the cluster nodes
   # gabconfig -a
   
 > maximum number of LLT links (including high and low priority) can a cluster have ?
   A cluster can have a maximum of 8 LLT links including high and low priority LLT links.
   
 > Check the detailed status of LLT links ?
   # lltstat -nvv
   
 > various LLT configuration files and their function
   LLT uses /etc/llttab to set the configuration of the LLT interconnects.
   
   # cat /etc/llttab
    set-node node01
    set-cluster 02
    link nxge1 /dev/nxge1 - ether - -
    link nxge2 /dev/nxge2 - ether - -
    link-lowpri /dev/nxge0 – ether - -
    
    Another configuration file used by LLT is – /etc/llthosts
    
    LLT has an another optional configuration file : /etc/VRTSvcs/conf/sysname. It contains short names for VCS to refer
    
  > various GAB configuration files and their function 
    file /etc/gabtab contains the command to start the GAB.
    
  > commands to start and stop GAB are 
    # gabconfig -c        (start GAB)
    # gabconfig -U        (stop GAB)
    
  > commands to stop and start LLT are 
    # lltconfig -c       -> start LLT
    # lltconfig -U       -> stop LLT (GAB needs to stopped first)
    
  > why manual GAB seeding is required ?
    The GAB configuration file /etc/gabtab defines the minimum number of nodes that must be communicating for the cluster to start. 
    This is called as GAB seeding.
    In case we don’t have sufficient number of nodes to start VCS [ may be due to a maintenance activity ], 
    but have to do it anyways, then we have do what is called as manual seeding by firing below command on each of the nodes.
    
    # gabconfig -c -x
    
  > various ways to stop HAD or VCS cluster
    # hastop -local
    # hastop -local -evacuate
    # hastop -local -force
    # hastop -all -force
    # hastop -all
    
  > list the resource dependencies 
    # hares -dep
    
  > Enable/disable a resource 
    # hares -modify [resource_name] Enabled 1      (To enable a resource)
    # hares -modify [resource_name] Enabled 0        (To disable a resource)
    
  > List all the parameters of a resource 
    # hares -display [resource]
 
  > add a service group(a general method)
    To add a service group named SG with 2 nodes (node01 and node02)
    
    
