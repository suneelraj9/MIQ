> Remove a failed disk using luxadm and cfgadm
  1. luxadm ( most SAS and SCSI disks )
  2. cfgadm ( most fiber channel disk )
  
  > Remove the disk using the luxadm command 
    # /usr/sbin/luxadm remove_device /dev/rdsk/c1t1d0s2
    
  > If the disk fails to get removed, physically remove the disk and use below command 
    # luxadm -e offline /dev/rdsk/c1t1d0s2
    
  > Cleanup the device files for the removed device from /dev directory 
    # devfsadm -Cv

SVM
****

  Replace failed root disk under Solaris Volume Manager (SVM)
  ***********************************************************
  
    1. Detach the sub-mirrors d11 and d1
       # metadetach d10 d11
       # metadetach d0 d1
       
    2. Next is to remove any metadb on the failed disk. Check for the metadbs configured in the system
       #metadb
       # metadb -d c1t0d0s7
       
    3. Remove the disk from the OS before physically removing it from the server.
       #luxadm remove_device <device name>
       if not worked
       #luxadm -e offline <device name>
       or
       cfgadm -c unconfigure <apid>
    
    4. Now add the new disk and run devfsadm to scan for the newly added disk.
       # devfsadm
       
    5. Once the disk is detected we need to copy the VTOC table from the c1t0d0 to the newly added disk 
       using the fmthard command :
       # prtvtoc /dev/rdsk/c1t0d0s2 | fmthard -s - /dev/rdsk/c1t1d0s2
       
    6. Install the bootblk on slice 0 of the new disk.
       # installboot /usr/platform/sun4u/lib/fs/ufs/bootblk /dev/rdsk/c1t1d0s0
       
    7. Add the metadbs we deleted.
       # metadb -afc 3 c1t0d0s7
    8. Update the device ID in the SVM database.
       # metadevadm -u c1t0d0
  
	> Boot from mirror
	  ok devalias
	  sds-mirror               /pci@1f,4000/scsi@3/disk@1,0
	  sds-root                 /pci@1f,4000/scsi@3/disk@0,0
	  net                      /pci@1f,4000/network@1,1
	  disk                     /pci@1f,4000/scsi@3/disk@0,0
	  cdrom                    /pci@1f,4000/scsi@3/disk@6,0:f
	  
   9. Attach the detach sub mirrors using the metaattach command.
      # metattach d10 d11
      # metattach d0 d1
      
   10. In case you did not detach the sub mirrors in step 1, 
       run the metereplace command to start the sync which will return the sub mirrors to the ‘Okay’ state:
       
       # metareplace -e d0  c1t0d0s0
	   d0: device c1t0d0s0 is enabled
       # metareplace -e d10  c1t0d0s1
	   d10: device c1t0d0s1 is enabled
	  
**********************************************************
**********************************************************
Solaris 10 patching with SVM 
****************************

1. Check the health of all metadatabase replicas (metadbs) and SVM metadevices	  
	# metastat
	# metastat -c
	# metadb -i
	
2. Check the current boot device.
	# prtconf -vp | grep -i boot
	bootpath: '/pci@0,0/pci15ad,1976@10/sd@0,0:a' 
	boot-device: 'pci@0,0/pci15ad,1976@10/sd@0,0:a disk0:a'
	
3. Confirm the boot disk and mirror disk from the format output.
  # echo | format
	Searching for disks...done

	AVAILABLE DISK SELECTIONS:
				 0. c1t0d0 
						/pci@0,0/pci15ad,1976@10/sd@0,0
				 1. c1t1d0 
						/pci@0,0/pci15ad,1976@10/sd@1,0
	Specify disk (enter its number): Specify disk (enter its number):
	
	As seen above, here the root disk is c1t0d0 and the mirror rootdisk is c1t1d0.
	
4. Install the bootblock in the root mirror disk c1t1d0, to make sure it is bootable.
	# installboot /usr/platform/`uname -i`/lib/fs/ufs/bootblk /dev/rdsk/c1t1d0
	
5. Detach and delete the SVM mirrors
	# metadetach -f d10 d12       ## detach root mirror
	# metadetach -f d20 d22       ## detach swap mirror
	# metaclear d12
	# metaclear d22
	
6. Clear the metadatabase replicas from the root mirror disk.
	# metadb -d /dev/dsk/c1t1d0sX 

7. Mount the mirror root disk and replace the SVM related entries in the /etc/vfstab and /etc/system to prevent SVM 
   to start on boot from the root mirror disk.
	 # mount /dev/dsk/c1t1d0 /mnt
	 # vi /etc/fstab
   Remove the entries related to SVM from the /etc/system file.
	 
8. Confirm if the server boots from the un-encapsulated SVM root mirror disk.
	 # init 0
   ok> boot rootmirror      (root mirror is a devalias at OBP)
9. Boot the server into single user mode.
   # init 0
   ok> boot -s
	 
10. Unzip the patchset bundle and look for the passcode.
   # unzip -q sol10_Recommended.zip
   # grep PASSCODE sol10_Recommended/sol10_Recommended.README
	 
11. Install the patch cluster
	  # ./installcluster --[passcode]
		
	  #init 6
		
12. Reattaching the mirror disk
		# metadb -a -c 3 c1t1d0s7
	  # metainit d12 1 1 c1t1d0s0
		# metainit d22 1 1 c1t1d0s1
		# metattach d10 d12
		# metattach d20 d22
		# metastat | grep -i sync   (check sync status)
		
		
Rollback
**********
1. Boot from the un-encapsulated SVM root mirror disk
	# init 0
	ok> boot rootmirror
	
2. Copy the partition table from rootmirror disk to root disk.
	# prtvtoc /dev/rdsk/c1t1d0s2 | fmthard -s - /dev/rdsk/c1t0d0s2

3. Create the state database replicas(metadbs).
	# metadb -afc 3 c1t1d0s7 c1t0d0s7
	
4. Create the sub-mirror metadevices.
	# metainit -f d11 1 1 c1t1d0s0
	# metainit -f d12 1 1 c1t0d0s0
	
5. Create the mirror d10 for root.
	# metainit d10 -m d11
	# metaroot d10
	
6. Add the below entry in the /etc/system file
	vi /etc/system
	set md:mirrored_root_flag = 1
	
7. Reboot the system
	# init 6
	
8. Attach the sub-mirror d12 to mirror d10 to sync the data.

	# metattach d10 d12

9. Re-create swap under root partition.
	# swap -l
	swapfile             dev  swaplo blocks   free
	/dev/dsk/c1t1d0s1   30,1       8 1548280 1548280
	# swap -d /dev/dsk/c1t1d0s1
	# metainit d22 1 1 c1t0d0s1
	# metainit d21 1 1 c1t1d0s1
	# metainit d20 -m d21
	# metattach d20 d22

	Change the /etc/vfstab entry for the new swap.
	/dev/dsk/c1t1d0s1      -       -       swap    -       no      -
	to:
	/dev/md/dsk/d20        -       -       swap    -       no      -
	
		Add the swap again and set as dump device.
		
		# swap -a /dev/md/dsk/d20
	# dumpadm -d swap
	# eeprom "nvramrc=devalias rootmirror /devices/pci@0,0/pci15ad,1976@10/sd@0,0:a rootdisk /devices/pci@0,0/pci15ad,1976@10/sd@1,0:a"
	
	11. Install the bootblock on the new mirror disk.
		# installboot /usr/platform/`uname -i`/lib/fs/ufs/bootblk /dev/rdsk/c1t0d0s0
		
		************************************************************************
		********************************************************************
	
	ok boot sds-mirror
	
	# metadb -i
	# metadb -d c0t0d0s5 c0t0d0s6
	# metadb -i
	
	# reboot -- sds-mirror
	
	Replace failed disk and restore redundancy
	********************************************
	
    prtvtoc /dev/rdsk/c0t1d0s2 | fmthard -s - /dev/rdsk/c0t0d0s2
    installboot /usr/platform/sun4u/lib/fs/ufs/bootblk /dev/rdsk/c0t0d0s0
    metadb -f -a /dev/dsk/c0t0d0s5
    metadb -f -a /dev/dsk/c0t0d0s6

    metareplace -e d0 c0t0d0s0
	
	Solaris SVM: Recovery procedures when BOTH sides of the mirror indicate a"Last Erred" state
	*********************************************************************************************
	> EXAMPLE 1 – metastat of a simple mirrored metadevice:
	  Note in this example, both submirrors are in a “Last Errd” state making normal recovery procedures impossible.
	  
    Determine which submirrors *if any* are valid. The goal is to locate 1 good submirror from each metadevice. 
	Once the good submirror has been located, recreate the mirror using the good submirror. 
	Attach the remaining submirror and mount the mirror to its original mount point.

    Below is an example of the procedure used to recover metadevice d6  (the procedure is identical for metadevice d14)	  
	
      # umount /dev/md/dsk/d6 (main mirror) 
      # metaclear -f d6 
      # fsck /dev/md/rdsk/d41 (sub mirror)
      # fsck /dev/md/rdsk/d42 (sub mirror)
      # mount /dev/md/dsk/d41 /
   	** verify data at this point ** If it not recovered from will need to mount d42 and verify the data
	
	If data is determined to be valid, recreate the mirror metadevice using this submirror

    	# umount /dev/md/rdsk/d41 
      # metainit d6 -m d41

	attach second submirror and mount metadevice

      # metattach d6 d42 
      # mount /dev/md/dsk/d6 /
	
	> SVM error messages
	*********************
	    https://docs.oracle.com/cd/E19455-01/806-3204/6jccb3gak/index.html
	
	> Find booth device
	********************
    root@ivmprod /$ prtconf -pv |grep -i bootpath
          bootpath:  '/pci@1c,600000/scsi@2/disk@0,0:a'
    root@ivmprod /$ 
	> Collect disk controller information: 
	*************************************
      root@ivmprod /$ cfgadm -al
      Ap_Id                     Type         Receptacle   Occupant     Condition
      c1                        scsi-bus     connected    configured   unknown
      c1::dsk/c1t0d0            disk         connected    configured   unknown
	
	> Check metadevice status: 
    **************************
      root@ivmprod /$ metastat -t |grep Maintenance
        c1t0d0s0    0  No     Maintenance             Mon Jun 23 20:08:18 2014
        c1t0d0s1    0  No     Maintenance             Wed Jun 25 11:00:10 2014
      root@ivmprod /$
	
	> Check metadb status: 
	**********************
    root@ivmprod /$ metadb -i
	
	> Collect metadevice information and make sure which are the metadevice we have to detach from mirroring:
	
      root@ivmprod /$ metastat -p
	
	> Detach failed disk from mirror / SVM: 
	
      root@ivmprod /$ metadetach d10 d11
	
	> Above we got error so doing forcefully: 
	
      root@ivmprod /$ metadetach -f d10 d11
      d10: submirror d11 is detached
      root@ivmprod /$
	
	> Check the status of detached disks: 
	
      root@ivmprod /$ metastat -p
	
	> Delete faulted metadb/replica devices: 
	
      root@ivmprod /$ metadb -d /dev/dsk/c1t0d0s7
      root@ivmprod /$ metadb -i
	
	> Clear detached metadevices and check the status: 
	
      root@ivmprod /$ metaclear d11 d21 d31

      root@ivmprod /$ metastat -p
	
	> Unconfigure faulty disk from OS : 
	
	root@ivmprod /$ cfgadm -c unconfigure c1::dsk/c1t0d0
	
    Now the time to inform our DC engineer to pull out the faulty disk and insert new disk. 
    Get confirmation from DC engineer before configure new disk in to OS level. 
    Also we can see the console logs to confirm whether the disk has been replaced successfully or not.
    Below is the console logs for my case: 
	
	> Configure newly added disk from OS:
	
    root@ivmprod /$ cfgadm -c configure c1::dsk/c1t0d0
    root@ivmprod /$ cfgadm -al
	
	> Copy VTOC from secondary disk to primary disk which we replaced: 
	
    prtvtoc /dev/rdsk/c1t1d0s2 |fmthard -s- /dev/rdsk/c1t0d0s2
	
	> Create metadb with three replica on slice 7(s7) to new disk: 
	
    root@ivmprod /$ metadb -afc 3 /dev/rdsk/c1t0d0s7
    root@ivmprod /$ metadb -i
	
	> Create medadevice using metainit command: 
	
    root@ivmprod /$ metainit d11 1 1 c1t0d0s0
    d11: Concat/Stripe is setup
    root@ivmprod /$ metainit d21 1 1 c1t0d0s1
    d21: Concat/Stripe is setup
    root@ivmprod /$ metainit d31 1 1 c1t0d0s6
    d31: Concat/Stripe is setup
    root@ivmprod /$
	
	> Attach newly created metadevice to main metadevice: 
	
    root@ivmprod /$ metattach d10 d11
    d10: submirror d11 is attached
    root@ivmprod /$ metattach d20 d21
    d20: submirror d21 is attached
    root@ivmprod /$ metattach d30 d31
    d30: submirror d31 is attached
    root@ivmprod /$
	
	> Check the syncing status: 
	
    root@ivmprod /$ metastat |grep %


VXVM
******
>> Veritas Naming Scheme
  1. Operating system-based naming scheme
  2. Enclosure based naming scheme.
  
  bash-3.00# vxddladm get namingscheme
  NAMING_SCHEME   PERSISTENCE   LOWERCASE       USE_AVID
  ======================================================
  Enclosure Based      Yes        Yes            Yes
  bash-3.00#
  
>> To change the Operating system based naming scheme
   bash-3.00# vxddladm set namingscheme=osn
   bash-3.00# vxdisk list
   DEVICE    TYPE     DISK    GROUP      STATUS
   c1t0d0s2 auto:ZFS   -       -       ZFS
   c1t3d0   auto:none  -       -       online invalid
   c1t4d0   auto:ZFS   -       -       ZFS
   c1t5d0   auto:none  -       -       online invalid
   
>> You can match the OS native names(i.e format) with VXVM enclose based names using the below command
   bash-3.00# vxdisk -e list
   DEVICE  TYPE   DISK   GROUP STATUS  OS_NATIVE_NAME ATTR
   disk_0  auto:none    -   -  online invalid  c1t3d0     -
   disk_1  auto:none    -   -  online invalid  c1t5d0     -
   disk_2  auto:ZFS     -   -  ZFS     c1t4d0     -
   disk_3  auto:ZFS     -   -  ZFS     c1t0d0s2   -

 >> Creating the new disk group
    #vxdg init UXDG UXDISK1=disk_0 UXDISK2=disk_1
    #vxdisk list
    DEVICE     TYPE             DISK     GROUP   STATUS
    disk_0     auto:cdsdisk    UXDISK1   UXDG    online
    disk_1     auto:cdsdisk    UXDISK2   UXDG    online
    
 >> After scanning the disk in OS,Scan the disks in veritas level by using the below command.
    bash-3.00# vxdisk scandisks
    
 >> Bringing the disk in  to veritas control
    bash-3.00# /etc/vx/bin/vxdisksetup -i disk_0
    
 >> If you want to setup the disk in specific format,you can use the below syntax.
    #vxdisksetup -i disk_XX format=cds or simple or sliced
    
 >> Addition of disk
    #vxdg -g UXDG adddisk UXDISK3=disk_4
 
 >> Removing the disk
    #vxdg -g UXDG rmdisk UXDISK3
 
 >> Maximum size of volume
    #vxassist -g UXDG maxsize
    Maximum volume size: 284672 (139Mb)
    
    #vxassist -g UXDG maxsize layout=mirror
    Maximum volume size: 10240 (60Mb)
    
    #vxassist -g UXDG maxsize layout=stripe
    Maximum volume size: 284672 (139Mb)
    
 >> Deporting diskgroup
    #vxdg list
    NAME      STATE        ID
    UXDG      enabled,cds       1364022395.37.sfos
    
    #vxdg deport UXDG
    By default vxdisk output will not show the deported diskgroup tag
    
    #vxdisk list
    DEVICE    TYPE           DISK  GROUP  STATUS
    disk_0    auto:cdsdisk    -     -     online
    disk_1    auto:cdsdisk    -     -     online
    disk_2    auto:ZFS        -     -     ZFS
    disk_3    auto:ZFS        -     -     ZFS
    disk_4    auto:cdsdisk    -     -     online
    disk_5    auto:cdsdisk    -     -     online
    disk_6    auto:cdsdisk    -     -     online
    
  >> To find the deported diskgroup disks
    #vxdisk -o alldgs list
    DEVICE    TYPE           DISK   GROUP  STATUS
    disk_0    auto:cdsdisk    -     (UXDG) online
    disk_1    auto:cdsdisk    -     (UXDG) online
    
  >> Importing the deported diskgroup
    #vxdg import UXDG
    #vxdg list
    NAME     STATE   ID
    UXDG     enabled,cds  1364022395.37.sfos
   
   Sometimes you may need to use -C flag to import the cluster diskgroup to clear the VCS lock.

   # vxdg -C import DG_NAME
   
 >> Re-Naming the Diskgroup
    #vxdg deport UXDG
    #vxdg -n NEWDG import UXDG
    
 >> Diskgroup configuration backup/restore:
    # ls -lrt /etc/vx/cbr/bk
    total 6
    drwxr-xr-x  14 root  root 18 Mar 27 19:18 UXDG.1364022395.37.sfos
    drwxr-xr-x   2 root  root  6 Mar 27 21:21 UXDG.1364398489.45.sfos
    
 >> To take current configuration backup
    # /etc/vx/bin/vxconfigbackup
    
 >> To take the current configuration to specific location
     # /etc/vx/bin/vxconfigbackup -l /var/tmp
    Start backing up diskgroup UXDG to /var/tmp/UXDG.1364398489.45.sfos.......
    VxVM  NOTICE V-5-2-3100 Backup complete for diskgroup: UXDG
    # ls -lrt /var/tmp/UXDG.1364398489.45.sfos
    total 48473 
    -rw-r--r-- 1 root root Mar28 17:38 1364398489.45.sfos.diskinfo
    -rw-r--r-- 1 root root Mar28 17:38 1364398489.45.sfos.cfgrec
    -rw-r--r-- 1 root root Mar28 17:39 1364398489.45.sfos.binconfig
    -rw-r--r-- 1 root root Mar28 17:39 1364398489.45.sfos.dginfo
    
 >> Display the configuration file in readable format
    #cd /etc/vx/cbr/bk/UXDG.1364474089.57.sfos
    # cat 1364474089.57.sfos.cfgrec |vxprint -D - -htr
    Disk group: UXDG
    dg UXDG  default      default  25000    1364474089.57.sfos
    dm UXDISK1      disk_0       auto     65536    143056   -
    dm UXDISK2      disk_1       auto     65536    143056   -
    
 >> To restore the diskgroup configuration
    To pre-commit the changes run the following  
    #/etc/vx/bin/vxconfigrestore -p UXDG

    To commit the changes after the precommit 
    #/etc/vx/bin/vxconfigrestore -c UXDG

    To abort the pre-commit: 
    #/etc/vx/bin/vxconfigrestore -d UXDG
    
 >> To confirm that disk group is ok:   
    #vxprint -g UXDG
    
 >> Creating new volume
    Syntax:
    #vxassist -g diskgroup make vol_name size layout=format disk-name
    
    > Concatenation volume
      # vxassist -g UXDG make uxoravol1 50M
    > If you want to make the volume using the specific disk
      bash-3.00# vxassist -g UXDG make uxoravol1 50M UXDISK1
    > use mkfs to create vxfs filesystem
      # mkfs -F vxfs /dev/vx/rdsk/UXDG/uxoravol1
    > create a new mountpoint and mount
      mount -F vxfs /dev/vx/dsk/UXDG/uxoravol1 /uxoravol1
      
  >> Striped volume
     #vxassist -g UXDG make urstripe 100M UXDISK1 UXDISK2 UXDISK3 layout=stripe  stripeunit=32k
     
  >> Mirror volume
     # vxassist -g UXDG make uxvol2 50M layout=mirror
     
     volume with two plex – one way mirror
     volume with three plex – two way mirror.
     
  >> Mirrored-stripe or RAID-0+1(stripping + mirroring)
     # vxassist -g UXDG make msvol 50M layout=mirror-stripe
     
  >> Striped-mirror or RAID- 1+0 (mirroring +stripping)- Layered volume
     # vxassist -g UXDG make smvol 50M layout=stripe-mirror
     
  >> RAID-5 (striping with parity)
     # vxassist -g UXDG make smvol 50M layout=raid5
     
  >> Removing volume
     # Un-mount the volume
     # vxassist -g UXDG remove volume smvol
     
  >> Volume resize
     > Determining how much space we can increase the volume
       # vxassist -g UXDG maxsize layout=mirror
       Maximum volume size: 71680(35Mb)
     > Resize the volume & filesystem
       # /etc/vx/bin/vxresize -g UXDG  smvol +10M
       Decreasing the volume and filesystem using vxresize
       # /etc/vx/bin/vxresize -g UXDG  smvol -10M
       
   >> Volume online relayout
      > Converting a concatenation volume to mirror volume
        Adding the mirror
        # vxassist -g UXDG mirror uxoravol1 &
        We can use & or -b option in vxassist to move the task in background
        
      > We can see active mirror tasks using below commands
        # vxtask list
        # vxtask monitor
       
      > Converting concatenate volume to concat-mirror
        # vxassist -g UXDG mirror concatv1
        # vxassist -g UXDG convert concatv1 layout=concat-mirror
        
      > Converting concatenate volume to mirror-stripe
        #vxassist -g UXDG relayout concatv1 layout=mirror-stripe
        # vxassist -g UXDG convert concatv1 layout=mirror-stripe
        
      > Converting concatenate volume to raid5
        # vxassist -g UXDG relayout concatv1 layout=raid5
        
      > Converting concatenate volume to stripe
        #vxassist -g UXDG relayout concatv1 layout=stripe
        
      > Converting concatenate volume to stripe-mirror
        # vxassist -g UXDG relayout concatv1 layout=stripe-mirror
        
      > To monitor the relayout status
        # vxrelayout status volume
        
  >> VXDMP(Dynamic Multi Pathing)
  
     > easiest way to check whether DMP is enabled or not
        # vxdisk path
     > Another way is to check multipathing at a single disk
        # vxdisk list c2t0d0s2
        
     > List all the DMP nodes under all enclosures
        # vxdmpadm list dmpnode all
        
     > Alternatively we can also use the below command to check the multipathing
       # vxdmpadm getsubpaths dmpnodename=devicename
       
     > To find what are Disk are using controller c1
        bash-3.00# vxdmpadm getsubpaths ctlr=c1
        NAME STATE[A] PATH-TYPE[M] DMPNODENAME ENCLR-TYPE ENCLR-NAMEATTRS
        ======================================================
        c1t3d0 ENABLED(A)    -      disk_0     Disk     disk           -
        c1t5d0 ENABLED(A)    -      disk_1     Disk     disk           -
        c1t4d0 ENABLED(A)    -      disk_2     Disk     disk           -
     
  
     > To list the controllers
       # vxdmpadm listctlr all
        CTLR-NAME       ENCLR-TYPE      STATE      ENCLR-NAME
        =====================================================
        c2              EMC             ENABLED      emc0
        c4              EMC             ENABLED      emc0
        c0              Disk            ENABLED      disk
        c1              Disk            ENABLED      disk
      
     > To list the enclosures
       # vxdmpadm listenclosure all
        ENCLR_NAME ENCLR_TYPE  ENCLR_SNO  STATUS ARRAY_TYPE LUN_COUNT
        ==============================================================
        emc0         EMC     000292704216 CONNECTED    A/A    331
        disk         Disk    DISKS        CONNECTED    Disk   4
        
     > To list all the DMP nodes under a specific enclosure 
       # vxdmpadm getdmpnode enclosure=enc0

        NAME     STATE   ENCLR-TYPE PATHS ENBL DSBL ENCLR-NAME
        =========================================================
        c2t1d0s2 ENABLED T300        2     2    0     enc0
        c2t1d1s2 ENABLED T300        2     2    0     enc0
        c2t1d2s2 ENABLED T300        2     2    0     enc0
        c2t1d3s2 ENABLED T300        2     2    0     enc0
        
      > To get information about a particula DMP node 
        # vxdmpadm getdmpnode nodename=c0t5006016041E03B33d0s2

        NAME              STATE    ENCLR-TYPE PATHS ENBL DSBL  ENCLR-NAME
        ====================================================================
        emc_clariion0_16 ENABLED EMC_CLARiiON  6     6    0    emc_clariion0
        
      > To disable controller
        # vxdmpadm disable ctlr=c2
        
      > To enable controller
        # vxdmpadm enable ctlr=c2
        
      > To start gathering the statistics
        # vxdmpadm iostat start
        
      > To see the statistics
        # vxdmpadm iostat show all
        
      > Display I/O statistics on specific pathname, DMP nodename and enclosure name
        # vxdmpadm iostat show pathname=c3t115d0s2
        # vxdmpadm iostat show dmpnodename=c0t0d0s2
        # vxdmpadm iostat show enclosure=Disk
        
      > reset the statistics to being the monitoring
        vxdmpadm iostat reset
        
        
      > To stop DMP
        # vxdmpadm stop restore
        
      > To start DMP,
        # vxdmpadm start restore

> Replacing a failed disk under VxVM
  # vxprint -htg mydg
   pl myvol-02     myvol        DISABLED NODEVICE 204800   CONCAT    -        WO
   
  # vxdisk -eoalldgs list
  -            -         disk02       mydg         failed was:c1t3d0s2
  
  # vxdiskadm
   5      Replace a failed or removed disk
   
 > Enable the VxVM aliases for root disk and root mirror
   # eeprom use-nvramrc?=true
   ok> setenv use-nvramrc? true
   
 > set the boot device order either from OS or from ok prompt.
   # eeprom boot-device=rootdisk rootmirror
   ok> setenv boot-device rootdisk rootmirror
   
 > To boot from mirror disk
   ok> boot rootmirror
   or
   # eeprom nvramrc=devalias rootmirror
   
 > Test the boot disk failure
   Disable the plex rootvol-01
  # vxmend -g rootdg off rootvol-01
 
 > To restore the plex state to original 
   ￼vxmend -g rootdg on rootvol-01
   
 > Unencapsulating the root disk
   # ￼vxassist -g rootdg remove mirror volume rootvol swapvol
   # vxunroot
   # shutdown -i6 -g0 -y
 
     
 *************************************************************
                  VCS-Veritas Cluster
 *************************************************************
 
 VCS
 ****
   > Failover – Service group runs on one system at a time.
   > Parallel – Service group runs on multiple systems simultaneously
   > Hybrid – SG behaves as Failover within the local cluster and Parallel for the remote cluster.
   
   > VCS main configuration file located -> /etc/VRTSvcs/conf/config/main.cf
   
   > set VCS configuration file (main.cf) ro/rw
     # haconf -dump -makero 
     # haconf -makerw   
     
   > VCS engine log file located -> /var/VRTSvcs/log/engine_A.log.
     #hamsg engine_A
     
   > check the status of the entire cluster
    #hastatus -sum
    
   > verify the syntax of the main.cf 
     # hacf -verify /etc/VRTSvcs/conf/config
     
 Different resource types
 *************************
     1. Persistent : VCS can only monitor these resources but can not offline or online them.
     2. On-Off : VCS can start and stop On-Off resource type. Most resources fall in this category.
     3. On-Only : VCS starts On-Only resources but does not stop them.Example:NFS
     
 steps involved in Offline VCS configuration
 ********************************************
    # haconf -dump -makero
    # hastop -all
    # cp -p /etc/VRTSvcs/conf/config/main.cf /etc/VRTSvcs/conf/config/main.cf_17march
    # vi /etc/VRTSvcs/conf/config/main.cf
    # hacf -verify /etc/VRTSvcs/conf/config/
    # hastart
    
 GAB, LLT and HAD 
 *****************
   Basic building blocks of vcs functionality.
   LLT (low latency transport protocol) – LLT transmits the heartbeats over the interconnects.
   GAB (Group membership services and atomic broadcast) - overall cluster membership information by tracking the heartbeats sent over LLT interconnects
   HAD (High Availability daemon) – the main VCS engine which manages the agents and service group. It is in turn monitored by a daemon named hashadow
   
 > Check the status of various GAB ports on the cluster nodes
   # gabconfig -a
   
 > maximum number of LLT links (including high and low priority) can a cluster have ?
   A cluster can have a maximum of 8 LLT links including high and low priority LLT links.
   
 > Check the detailed status of LLT links ?
   # lltstat -nvv
   
 > various LLT configuration files and their function
   LLT uses /etc/llttab to set the configuration of the LLT interconnects.
   
   # cat /etc/llttab
    set-node node01
    set-cluster 02
    link nxge1 /dev/nxge1 - ether - -
    link nxge2 /dev/nxge2 - ether - -
    link-lowpri /dev/nxge0 – ether - -
    
    Another configuration file used by LLT is – /etc/llthosts
    
    LLT has an another optional configuration file : /etc/VRTSvcs/conf/sysname. It contains short names for VCS to refer
    
  > various GAB configuration files and their function 
    file /etc/gabtab contains the command to start the GAB.
    
  > commands to start and stop GAB are 
    # gabconfig -c        (start GAB)
    # gabconfig -U        (stop GAB)
    
  > commands to stop and start LLT are 
    # lltconfig -c       -> start LLT
    # lltconfig -U       -> stop LLT (GAB needs to stopped first)
    
  > why manual GAB seeding is required ?
    The GAB configuration file /etc/gabtab defines the minimum number of nodes that must be communicating for the cluster to start. 
    This is called as GAB seeding.
    In case we don’t have sufficient number of nodes to start VCS [ may be due to a maintenance activity ], 
    but have to do it anyways, then we have do what is called as manual seeding by firing below command on each of the nodes.
    
    # gabconfig -c -x
    
  > various ways to stop HAD or VCS cluster
    # hastop -local
    # hastop -local -evacuate
    # hastop -local -force
    # hastop -all -force
    # hastop -all
    
  > list the resource dependencies 
    # hares -dep
    
  > Enable/disable a resource 
    # hares -modify [resource_name] Enabled 1      (To enable a resource)
    # hares -modify [resource_name] Enabled 0        (To disable a resource)
    
  > List all the parameters of a resource 
    # hares -display [resource]
 
  > add a service group(a general method)
    To add a service group named SG with 2 nodes (node01 and node02)
    
    haconf –makerw
    haconf –makerw
    hagrp –add SG
    hagrp –modify SG SystemList node01 0 node02 1
    hagrp –modify SG AutoStartList node02
    haconf –dump -makero
    
  > check the configuration of a service group 
    # hagrp -display SG
    
  > bring service group online/offline
    # hagrp -online [service-group] -sys [node]      (Online the SG on a particular node)
    # hagrp -offline [service-group] -sys [node]        (Offline the SG on particular node)
    
    The -any option when used instead of the node name, brings the SG online/offline based on SG’s failover policy. 
    
    # hagrp -online [service-group] -any
    # hagrp -offline [service-group] -any
    
  > The command to switch the service group to target node :
    # hagrp -switch [service-group] -to [target-node]
    
  > freeze/unfreeze a service group and what happens when you do so
    When you freeze a service group, VCS continues to monitor the service group, but does not allow it or the resources under it to 
    be taken offline or brought online. 
    Failover is also disable even when a resource faults. When you unfreeze the SG, it start behaving in the normal way
    
   > To freeze/unfreeze a Service Group temporarily
      # hagrp -freeze [service-group]
      # hagrp -unfreeze [service-group]
      
   > To freeze/unfreeze a Service Group persistently (across reboots) :
      # hagrp -freeze -persistent[service-group]
      # hagrp -unfreeze [service-group] -persistent
    
   > How to clear resource faults ?
     For persistent resources :
     Do not do anything and wait for the next OfflineMonitorInterval (default – 300 seconds) for the resource to become online.
     
     For non-persistent resources 
     Clear the fault and probe the resource on node01
     # hares -clear [resource_name] -sys node01
      # hares -probe [resource_name] -sys node01
      
   > clear resources with ADMIN_WAIT state
     If the ManageFaults attribute of a service group is set to NONE, VCS does not take any automatic action when it detects 
     a resource fault. VCS places the resource into the ADMIN_WAIT state and waits for administrative intervention. 
     
     To clear the resource in ADMIN_WAIT state without faulting service group
     #hares -probe [resource] -sys node01
     
     To clear the resource in ADMIN_WAIT state by changing the status to OFFLINE|FAULTED 
     # hagrp -clearadminwait -fault [SG] -sys node01
     
   > flush a service group and when its required 
     Flushing of a service group is required when, agents for the resources in the service group seems suspended waiting for 
     resources to be taken online/offline. 
     Flushing a service group clears any internal wait states and stops VCS from attempting to bring resources online.
     
     # hagrp -flush [SG] -sys node01
     
 > Install and configure VCS on the new node
 	
	After you have finished installing the vcs software run installsf from the software media
	# cd /cdrom/VRTSvcs/
	# installsf -addnode
	
	SET OF QUESTIONS NEED TO ANSWER IT OUT
	
	Verify:
	Confirm the configuration: run these commands on node03
	# lltstat -nvv
	# cat /etc/llttab
	# cat /etc/llthosts
	# gabconfig -a
	# cat /etc/gabtab
	# vxfenadm -d
	# cat /etc/vxfenmode
	# cat /etc/VRTSvcs/conf/config/main.cf   (ClusterServices Sg should have node03)
	
 > Remove a node from an active VCS cluster
    
     Freeze node and halt VCS
     ************************
        # haconf -makerw
	# hasys -freeze -persistent node03
	# haconf -dump
	# hastop -sys node03 -evacuate
	
     Stop I/O fencing and stop LLT/GAB modules
     ******************************************
        # /sbin/vxfen-shutdown
	# vxfenadm -d     (check status of fencing from node01 or node02)
	# gabconfig -U
	# lltconfig -U
	
    > Remove VCS software
    	Remove VCS software from node03 by running the installer utility and rename the main.cf file.
	
    > Modify AutoStartList and SystemList
    	On node01
	# hagrp -modify SG01 AutoStartList –delete node03
	# hagrp -modify SG01 SystemList –delete node03
	On node02
	# hagrp -modify SG02 AutoStartList –delete node03
	# hagrp -modify SG02 SystemList –delete node03
 	
    > Remove node from ClusterServices SG
    	# hagrp -modify ClusterService AutoStartList –delete node03
	
    > Remove node from cluster
    	Now remove node03 from cluster and save the configuration(run this on node01 or node02)
	# hasys -delete train10
	# haconf -dump -makero
	
> Configure VCS I/O fencing – Command line and installer utility

    Before configuring the disks for fencing you can run a test to confirm whether they are SCSI3-PGR compatible disks. 
    The vxfentsthdw script guides you through to test all the disks to be configured in the fencing DG. 
    
    # vxfentsthdw
    # vxfentsthdw -c vxfencoorddg
    
    > Steps to configure I/O fencing  
      1. Initialize disks for I/O fencing
        # vxdisk -eo alldgs list
	# vxdisksetup -i disk01
	# vxdisksetup -i disk02
	# vxdisksetup -i disk03
	
    > Run the installvcs script from the install media with fencing option
        # cd /cdrom/VRTS/install
	# ./installvcs -fencing
	Cluster information verification: Cluster Name: geekdiary
	Cluster ID Number: 3
	Systems: node01 node02
	Would you like to configure I/O fencing on the cluster? [y,n,q] y
	
    >  Select disk based fencing
       We will be doing a disk based fencing rather than a server based fencing also called as CP (coordinator point) client based fencing.
    You can also directly give the fencing DG name to test an entire DG.
    
    OR usiN COMMAND LINE USE BELOW COMMANDS
