SVM
****

	> Boot from mirror
	  ok devalias
	  sds-mirror               /pci@1f,4000/scsi@3/disk@1,0
	  sds-root                 /pci@1f,4000/scsi@3/disk@0,0
	  net                      /pci@1f,4000/network@1,1
	  disk                     /pci@1f,4000/scsi@3/disk@0,0
	  cdrom                    /pci@1f,4000/scsi@3/disk@6,0:f
	  
	ok boot sds-mirror
	
	# metadb -i
	# metadb -d c0t0d0s5 c0t0d0s6
	# metadb -i
	
	# reboot -- sds-mirror
	
	Replace failed disk and restore redundancy
	********************************************
	
    prtvtoc /dev/rdsk/c0t1d0s2 | fmthard -s - /dev/rdsk/c0t0d0s2
    installboot /usr/platform/sun4u/lib/fs/ufs/bootblk /dev/rdsk/c0t0d0s0
    metadb -f -a /dev/dsk/c0t0d0s5
    metadb -f -a /dev/dsk/c0t0d0s6

    metareplace -e d0 c0t0d0s0
	
	Solaris SVM: Recovery procedures when BOTH sides of the mirror indicate a"Last Erred" state
	*********************************************************************************************
	> EXAMPLE 1 – metastat of a simple mirrored metadevice:
	  Note in this example, both submirrors are in a “Last Errd” state making normal recovery procedures impossible.
	  
    Determine which submirrors *if any* are valid. The goal is to locate 1 good submirror from each metadevice. 
	Once the good submirror has been located, recreate the mirror using the good submirror. 
	Attach the remaining submirror and mount the mirror to its original mount point.

    Below is an example of the procedure used to recover metadevice d6  (the procedure is identical for metadevice d14)	  
	
      # umount /dev/md/dsk/d6 (main mirror) 
      # metaclear -f d6 
      # fsck /dev/md/rdsk/d41 (sub mirror)
      # fsck /dev/md/rdsk/d42 (sub mirror)
      # mount /dev/md/dsk/d41 /
   	** verify data at this point ** If it not recovered from will need to mount d42 and verify the data
	
	If data is determined to be valid, recreate the mirror metadevice using this submirror

    	# umount /dev/md/rdsk/d41 
      # metainit d6 -m d41

	attach second submirror and mount metadevice

      # metattach d6 d42 
      # mount /dev/md/dsk/d6 /
	
	> SVM error messages
	*********************
	    https://docs.oracle.com/cd/E19455-01/806-3204/6jccb3gak/index.html
	
	> Find booth device
	********************
    root@ivmprod /$ prtconf -pv |grep -i bootpath
          bootpath:  '/pci@1c,600000/scsi@2/disk@0,0:a'
    root@ivmprod /$ 
	> Collect disk controller information: 
	*************************************
      root@ivmprod /$ cfgadm -al
      Ap_Id                     Type         Receptacle   Occupant     Condition
      c1                        scsi-bus     connected    configured   unknown
      c1::dsk/c1t0d0            disk         connected    configured   unknown
	
	> Check metadevice status: 
    **************************
      root@ivmprod /$ metastat -t |grep Maintenance
        c1t0d0s0    0  No     Maintenance             Mon Jun 23 20:08:18 2014
        c1t0d0s1    0  No     Maintenance             Wed Jun 25 11:00:10 2014
      root@ivmprod /$
	
	> Check metadb status: 
	**********************
    root@ivmprod /$ metadb -i
	
	> Collect metadevice information and make sure which are the metadevice we have to detach from mirroring:
	
      root@ivmprod /$ metastat -p
	
	> Detach failed disk from mirror / SVM: 
	
      root@ivmprod /$ metadetach d10 d11
	
	> Above we got error so doing forcefully: 
	
      root@ivmprod /$ metadetach -f d10 d11
      d10: submirror d11 is detached
      root@ivmprod /$
	
	> Check the status of detached disks: 
	
      root@ivmprod /$ metastat -p
	
	> Delete faulted metadb/replica devices: 
	
      root@ivmprod /$ metadb -d /dev/dsk/c1t0d0s7
      root@ivmprod /$ metadb -i
	
	> Clear detached metadevices and check the status: 
	
      root@ivmprod /$ metaclear d11 d21 d31

      root@ivmprod /$ metastat -p
	
	> Unconfigure faulty disk from OS : 
	
	root@ivmprod /$ cfgadm -c unconfigure c1::dsk/c1t0d0
	
    Now the time to inform our DC engineer to pull out the faulty disk and insert new disk. 
    Get confirmation from DC engineer before configure new disk in to OS level. 
    Also we can see the console logs to confirm whether the disk has been replaced successfully or not.
    Below is the console logs for my case: 
	
	> Configure newly added disk from OS:
	
    root@ivmprod /$ cfgadm -c configure c1::dsk/c1t0d0
    root@ivmprod /$ cfgadm -al
	
	> Copy VTOC from secondary disk to primary disk which we replaced: 
	
    prtvtoc /dev/rdsk/c1t1d0s2 |fmthard -s- /dev/rdsk/c1t0d0s2
	
	> Create metadb with three replica on slice 7(s7) to new disk: 
	
    root@ivmprod /$ metadb -afc 3 /dev/rdsk/c1t0d0s7
    root@ivmprod /$ metadb -i
	
	> Create medadevice using metainit command: 
	
    root@ivmprod /$ metainit d11 1 1 c1t0d0s0
    d11: Concat/Stripe is setup
    root@ivmprod /$ metainit d21 1 1 c1t0d0s1
    d21: Concat/Stripe is setup
    root@ivmprod /$ metainit d31 1 1 c1t0d0s6
    d31: Concat/Stripe is setup
    root@ivmprod /$
	
	> Attach newly created metadevice to main metadevice: 
	
    root@ivmprod /$ metattach d10 d11
    d10: submirror d11 is attached
    root@ivmprod /$ metattach d20 d21
    d20: submirror d21 is attached
    root@ivmprod /$ metattach d30 d31
    d30: submirror d31 is attached
    root@ivmprod /$
	
	> Check the syncing status: 
	
    root@ivmprod /$ metastat |grep %


VXVM
******
>> Veritas Naming Scheme
  1. Operating system-based naming scheme
  2. Enclosure based naming scheme.
  
  bash-3.00# vxddladm get namingscheme
  NAMING_SCHEME   PERSISTENCE   LOWERCASE       USE_AVID
  ======================================================
  Enclosure Based      Yes        Yes            Yes
  bash-3.00#
  
>> To change the Operating system based naming scheme
   bash-3.00# vxddladm set namingscheme=osn
   bash-3.00# vxdisk list
   DEVICE    TYPE     DISK    GROUP      STATUS
   c1t0d0s2 auto:ZFS   -       -       ZFS
   c1t3d0   auto:none  -       -       online invalid
   c1t4d0   auto:ZFS   -       -       ZFS
   c1t5d0   auto:none  -       -       online invalid
   
>> You can match the OS native names(i.e format) with VXVM enclose based names using the below command
   bash-3.00# vxdisk -e list
   DEVICE  TYPE   DISK   GROUP STATUS  OS_NATIVE_NAME ATTR
   disk_0  auto:none    -   -  online invalid  c1t3d0     -
   disk_1  auto:none    -   -  online invalid  c1t5d0     -
   disk_2  auto:ZFS     -   -  ZFS     c1t4d0     -
   disk_3  auto:ZFS     -   -  ZFS     c1t0d0s2   -

 >> Creating the new disk group
    #vxdg init UXDG UXDISK1=disk_0 UXDISK2=disk_1
    #vxdisk list
    DEVICE     TYPE             DISK     GROUP   STATUS
    disk_0     auto:cdsdisk    UXDISK1   UXDG    online
    disk_1     auto:cdsdisk    UXDISK2   UXDG    online
    
 >> After scanning the disk in OS,Scan the disks in veritas level by using the below command.
    bash-3.00# vxdisk scandisks
    
 >> Bringing the disk in  to veritas control
    bash-3.00# /etc/vx/bin/vxdisksetup -i disk_0
    
 >> If you want to setup the disk in specific format,you can use the below syntax.
    #vxdisksetup -i disk_XX format=cds or simple or sliced
    
 >> Addition of disk
    #vxdg -g UXDG adddisk UXDISK3=disk_4
 
 >> Removing the disk
    #vxdg -g UXDG rmdisk UXDISK3
 
 >> Maximum size of volume
    #vxassist -g UXDG maxsize
    Maximum volume size: 284672 (139Mb)
    
    #vxassist -g UXDG maxsize layout=mirror
    Maximum volume size: 10240 (60Mb)
    
    #vxassist -g UXDG maxsize layout=stripe
    Maximum volume size: 284672 (139Mb)
    
 >> Deporting diskgroup
    #vxdg list
    NAME      STATE        ID
    UXDG      enabled,cds       1364022395.37.sfos
    
    #vxdg deport UXDG
    By default vxdisk output will not show the deported diskgroup tag
    
    #vxdisk list
    DEVICE    TYPE           DISK  GROUP  STATUS
    disk_0    auto:cdsdisk    -     -     online
    disk_1    auto:cdsdisk    -     -     online
    disk_2    auto:ZFS        -     -     ZFS
    disk_3    auto:ZFS        -     -     ZFS
    disk_4    auto:cdsdisk    -     -     online
    disk_5    auto:cdsdisk    -     -     online
    disk_6    auto:cdsdisk    -     -     online
    
  >> To find the deported diskgroup disks
    #vxdisk -o alldgs list
    DEVICE    TYPE           DISK   GROUP  STATUS
    disk_0    auto:cdsdisk    -     (UXDG) online
    disk_1    auto:cdsdisk    -     (UXDG) online
    
  >> Importing the deported diskgroup
    #vxdg import UXDG
    #vxdg list
    NAME     STATE   ID
    UXDG     enabled,cds  1364022395.37.sfos
   
   Sometimes you may need to use -C flag to import the cluster diskgroup to clear the VCS lock.

   # vxdg -C import DG_NAME
   
 >> Re-Naming the Diskgroup
    #vxdg deport UXDG
    #vxdg -n NEWDG import UXDG
    
 >> Diskgroup configuration backup/restore:
    # ls -lrt /etc/vx/cbr/bk
    total 6
    drwxr-xr-x  14 root  root 18 Mar 27 19:18 UXDG.1364022395.37.sfos
    drwxr-xr-x   2 root  root  6 Mar 27 21:21 UXDG.1364398489.45.sfos
    
 >> To take current configuration backup
    # /etc/vx/bin/vxconfigbackup
    
 >> To take the current configuration to specific location
     # /etc/vx/bin/vxconfigbackup -l /var/tmp
    Start backing up diskgroup UXDG to /var/tmp/UXDG.1364398489.45.sfos.......
    VxVM  NOTICE V-5-2-3100 Backup complete for diskgroup: UXDG
    # ls -lrt /var/tmp/UXDG.1364398489.45.sfos
    total 48473 
    -rw-r--r-- 1 root root Mar28 17:38 1364398489.45.sfos.diskinfo
    -rw-r--r-- 1 root root Mar28 17:38 1364398489.45.sfos.cfgrec
    -rw-r--r-- 1 root root Mar28 17:39 1364398489.45.sfos.binconfig
    -rw-r--r-- 1 root root Mar28 17:39 1364398489.45.sfos.dginfo
    
 >> Display the configuration file in readable format
    #cd /etc/vx/cbr/bk/UXDG.1364474089.57.sfos
    # cat 1364474089.57.sfos.cfgrec |vxprint -D - -htr
    Disk group: UXDG
    dg UXDG  default      default  25000    1364474089.57.sfos
    dm UXDISK1      disk_0       auto     65536    143056   -
    dm UXDISK2      disk_1       auto     65536    143056   -
    
 >> To restore the diskgroup configuration
    To pre-commit the changes run the following  
    #/etc/vx/bin/vxconfigrestore -p UXDG

    To commit the changes after the precommit 
    #/etc/vx/bin/vxconfigrestore -c UXDG

    To abort the pre-commit: 
    #/etc/vx/bin/vxconfigrestore -d UXDG
    
 >> To confirm that disk group is ok:   
    #vxprint -g UXDG
    
 >> Creating new volume
    Syntax:
    #vxassist -g diskgroup make vol_name size layout=format disk-name
    
    > Concatenation volume
      # vxassist -g UXDG make uxoravol1 50M
    > If you want to make the volume using the specific disk
      bash-3.00# vxassist -g UXDG make uxoravol1 50M UXDISK1
    > use mkfs to create vxfs filesystem
      # mkfs -F vxfs /dev/vx/rdsk/UXDG/uxoravol1
    > create a new mountpoint and mount
      mount -F vxfs /dev/vx/dsk/UXDG/uxoravol1 /uxoravol1
      
  >> Striped volume
     #vxassist -g UXDG make urstripe 100M UXDISK1 UXDISK2 UXDISK3 layout=stripe  stripeunit=32k
     
  >> Mirror volume
     # vxassist -g UXDG make uxvol2 50M layout=mirror
     
     volume with two plex – one way mirror
     volume with three plex – two way mirror.
     
  >> Mirrored-stripe or RAID-0+1(stripping + mirroring)
     # vxassist -g UXDG make msvol 50M layout=mirror-stripe
     
  >> Striped-mirror or RAID- 1+0 (mirroring +stripping)- Layered volume
     # vxassist -g UXDG make smvol 50M layout=stripe-mirror
     
  >> RAID-5 (striping with parity)
     # vxassist -g UXDG make smvol 50M layout=raid5
     
  >> Removing volume
     # Un-mount the volume
     # vxassist -g UXDG remove volume smvol
     
  >> Volume resize
     > Determining how much space we can increase the volume
       # vxassist -g UXDG maxsize layout=mirror
       Maximum volume size: 71680(35Mb)
     > Resize the volume & filesystem
       # /etc/vx/bin/vxresize -g UXDG  smvol +10M
       Decreasing the volume and filesystem using vxresize
       # /etc/vx/bin/vxresize -g UXDG  smvol -10M
       
   >> Volume online relayout
      > Converting a concatenation volume to mirror volume
        Adding the mirror
        # vxassist -g UXDG mirror uxoravol1 &
        We can use & or -b option in vxassist to move the task in background
        
      > We can see active mirror tasks using below commands
        # vxtask list
        # vxtask monitor
       
      > Converting concatenate volume to concat-mirror
        # vxassist -g UXDG mirror concatv1
        # vxassist -g UXDG convert concatv1 layout=concat-mirror
        
      > Converting concatenate volume to mirror-stripe
        #vxassist -g UXDG relayout concatv1 layout=mirror-stripe
        # vxassist -g UXDG convert concatv1 layout=mirror-stripe
        
      > Converting concatenate volume to raid5
        # vxassist -g UXDG relayout concatv1 layout=raid5
        
      > Converting concatenate volume to stripe
        #vxassist -g UXDG relayout concatv1 layout=stripe
        
      > Converting concatenate volume to stripe-mirror
        # vxassist -g UXDG relayout concatv1 layout=stripe-mirror
        
      > To monitor the relayout status
        # vxrelayout status volume
        
  >> VXDMP(Dynamic Multi Pathing)
  
     > easiest way to check whether DMP is enabled or not
        # vxdisk path
     > Another way is to check multipathing at a single disk
        # vxdisk list c2t0d0s2
        
     > List all the DMP nodes under all enclosures
        # vxdmpadm list dmpnode all
        
     > Alternatively we can also use the below command to check the multipathing
       # vxdmpadm getsubpaths dmpnodename=devicename
       
     > To find what are Disk are using controller c1
        bash-3.00# vxdmpadm getsubpaths ctlr=c1
        NAME STATE[A] PATH-TYPE[M] DMPNODENAME ENCLR-TYPE ENCLR-NAMEATTRS
        ======================================================
        c1t3d0 ENABLED(A)    -      disk_0     Disk     disk           -
        c1t5d0 ENABLED(A)    -      disk_1     Disk     disk           -
        c1t4d0 ENABLED(A)    -      disk_2     Disk     disk           -
     
  
     > To list the controllers
       # vxdmpadm listctlr all
        CTLR-NAME       ENCLR-TYPE      STATE      ENCLR-NAME
        =====================================================
        c2              EMC             ENABLED      emc0
        c4              EMC             ENABLED      emc0
        c0              Disk            ENABLED      disk
        c1              Disk            ENABLED      disk
      
     > To list the enclosures
       # vxdmpadm listenclosure all
        ENCLR_NAME ENCLR_TYPE  ENCLR_SNO  STATUS ARRAY_TYPE LUN_COUNT
        ==============================================================
        emc0         EMC     000292704216 CONNECTED    A/A    331
        disk         Disk    DISKS        CONNECTED    Disk   4
        
     > To list all the DMP nodes under a specific enclosure 
       # vxdmpadm getdmpnode enclosure=enc0

        NAME     STATE   ENCLR-TYPE PATHS ENBL DSBL ENCLR-NAME
        =========================================================
        c2t1d0s2 ENABLED T300        2     2    0     enc0
        c2t1d1s2 ENABLED T300        2     2    0     enc0
        c2t1d2s2 ENABLED T300        2     2    0     enc0
        c2t1d3s2 ENABLED T300        2     2    0     enc0
        
      > To get information about a particula DMP node 
        # vxdmpadm getdmpnode nodename=c0t5006016041E03B33d0s2

        NAME              STATE    ENCLR-TYPE PATHS ENBL DSBL  ENCLR-NAME
        ====================================================================
        emc_clariion0_16 ENABLED EMC_CLARiiON  6     6    0    emc_clariion0
        
      > To disable controller
        # vxdmpadm disable ctlr=c2
        
      > To enable controller
        # vxdmpadm enable ctlr=c2
        
      > To start gathering the statistics
        # vxdmpadm iostat start
        
      > To see the statistics
        # vxdmpadm iostat show all
        
      > Display I/O statistics on specific pathname, DMP nodename and enclosure name
        # vxdmpadm iostat show pathname=c3t115d0s2
        # vxdmpadm iostat show dmpnodename=c0t0d0s2
        # vxdmpadm iostat show enclosure=Disk
        
      > reset the statistics to being the monitoring
        vxdmpadm iostat reset
        
        
      > To stop DMP
        # vxdmpadm stop restore
        
      > To start DMP,
        # vxdmpadm start restore
        
     
 *************************************************************
                  VCS-Veritas Cluster
 *************************************************************
 
 VCS
 ****
   > Failover – Service group runs on one system at a time.
   > Parallel – Service group runs on multiple systems simultaneously
   > Hybrid – SG behaves as Failover within the local cluster and Parallel for the remote cluster.
   
   > VCS main configuration file located -> /etc/VRTSvcs/conf/config/main.cf
   
   > set VCS configuration file (main.cf) ro/rw
     # haconf -dump -makero 
     # haconf -makerw   
     
   > VCS engine log file located -> /var/VRTSvcs/log/engine_A.log.
     #hamsg engine_A
     
   > check the status of the entire cluster
    #hastatus -sum
    
   > verify the syntax of the main.cf 
     # hacf -verify /etc/VRTSvcs/conf/config
     
 Different resource types
 *************************
     1. Persistent : VCS can only monitor these resources but can not offline or online them.
     2. On-Off : VCS can start and stop On-Off resource type. Most resources fall in this category.
     3. On-Only : VCS starts On-Only resources but does not stop them.Example:NFS
     
 steps involved in Offline VCS configuration
 ********************************************
    # haconf -dump -makero
    # hastop -all
    # cp -p /etc/VRTSvcs/conf/config/main.cf /etc/VRTSvcs/conf/config/main.cf_17march
    # vi /etc/VRTSvcs/conf/config/main.cf
    # hacf -verify /etc/VRTSvcs/conf/config/
    # hastart
    
 GAB, LLT and HAD 
 *****************
   Basic building blocks of vcs functionality.
   LLT (low latency transport protocol) – LLT transmits the heartbeats over the interconnects.
   GAB (Group membership services and atomic broadcast) - overall cluster membership information by tracking the heartbeats sent over LLT interconnects
   HAD (High Availability daemon) – the main VCS engine which manages the agents and service group. It is in turn monitored by a daemon named hashadow
   
 > Check the status of various GAB ports on the cluster nodes
   # gabconfig -a
   
 > maximum number of LLT links (including high and low priority) can a cluster have ?
   A cluster can have a maximum of 8 LLT links including high and low priority LLT links.
   
 > Check the detailed status of LLT links ?
   # lltstat -nvv
   
 > various LLT configuration files and their function
   LLT uses /etc/llttab to set the configuration of the LLT interconnects.
   
   # cat /etc/llttab
    set-node node01
    set-cluster 02
    link nxge1 /dev/nxge1 - ether - -
    link nxge2 /dev/nxge2 - ether - -
    link-lowpri /dev/nxge0 – ether - -
    
    Another configuration file used by LLT is – /etc/llthosts
    
    LLT has an another optional configuration file : /etc/VRTSvcs/conf/sysname. It contains short names for VCS to refer
    
  > various GAB configuration files and their function 
    file /etc/gabtab contains the command to start the GAB.
    
  > commands to start and stop GAB are 
    # gabconfig -c        (start GAB)
    # gabconfig -U        (stop GAB)
    
  > commands to stop and start LLT are 
    # lltconfig -c       -> start LLT
    # lltconfig -U       -> stop LLT (GAB needs to stopped first)
    
  > why manual GAB seeding is required ?
    The GAB configuration file /etc/gabtab defines the minimum number of nodes that must be communicating for the cluster to start. 
    This is called as GAB seeding.
    In case we don’t have sufficient number of nodes to start VCS [ may be due to a maintenance activity ], 
    but have to do it anyways, then we have do what is called as manual seeding by firing below command on each of the nodes.
    
    # gabconfig -c -x
    
  > various ways to stop HAD or VCS cluster
    # hastop -local
    # hastop -local -evacuate
    # hastop -local -force
    # hastop -all -force
    # hastop -all
    
  > list the resource dependencies 
    # hares -dep
    
  > Enable/disable a resource 
    # hares -modify [resource_name] Enabled 1      (To enable a resource)
    # hares -modify [resource_name] Enabled 0        (To disable a resource)
    
  > List all the parameters of a resource 
    # hares -display [resource]
 
  > add a service group(a general method)
    To add a service group named SG with 2 nodes (node01 and node02)
    
    haconf –makerw
    haconf –makerw
    hagrp –add SG
    hagrp –modify SG SystemList node01 0 node02 1
    hagrp –modify SG AutoStartList node02
    haconf –dump -makero
    
  > check the configuration of a service group 
    # hagrp -display SG
    
  > bring service group online/offline
    # hagrp -online [service-group] -sys [node]      (Online the SG on a particular node)
    # hagrp -offline [service-group] -sys [node]        (Offline the SG on particular node)
    
    The -any option when used instead of the node name, brings the SG online/offline based on SG’s failover policy. 
    
    # hagrp -online [service-group] -any
    # hagrp -offline [service-group] -any
    
  > The command to switch the service group to target node :
    # hagrp -switch [service-group] -to [target-node]
    
  > freeze/unfreeze a service group and what happens when you do so
    When you freeze a service group, VCS continues to monitor the service group, but does not allow it or the resources under it to 
    be taken offline or brought online. 
    Failover is also disable even when a resource faults. When you unfreeze the SG, it start behaving in the normal way
    
   > To freeze/unfreeze a Service Group temporarily
      # hagrp -freeze [service-group]
      # hagrp -unfreeze [service-group]
      
   > To freeze/unfreeze a Service Group persistently (across reboots) :
      # hagrp -freeze -persistent[service-group]
      # hagrp -unfreeze [service-group] -persistent
    
   > How to clear resource faults ?
     For persistent resources :
     Do not do anything and wait for the next OfflineMonitorInterval (default – 300 seconds) for the resource to become online.
     
     For non-persistent resources 
     Clear the fault and probe the resource on node01
     # hares -clear [resource_name] -sys node01
      # hares -probe [resource_name] -sys node01
      
   > clear resources with ADMIN_WAIT state
     If the ManageFaults attribute of a service group is set to NONE, VCS does not take any automatic action when it detects 
     a resource fault. VCS places the resource into the ADMIN_WAIT state and waits for administrative intervention. 
     
     To clear the resource in ADMIN_WAIT state without faulting service group
     #hares -probe [resource] -sys node01
     
     To clear the resource in ADMIN_WAIT state by changing the status to OFFLINE|FAULTED 
     # hagrp -clearadminwait -fault [SG] -sys node01
     
   > flush a service group and when its required 
     Flushing of a service group is required when, agents for the resources in the service group seems suspended waiting for 
     resources to be taken online/offline. 
     Flushing a service group clears any internal wait states and stops VCS from attempting to bring resources online.
     
     # hagrp -flush [SG] -sys node01
